Host Examples
==================================
OpenCL host code for optimized interfacing with Xilinx Devices.

 __Examples Table__ 

Example        | Description           | Key Concepts / Keywords 
---------------|-----------------------|-------------------------
[concurrent_kernel_execution/][]|This example will demonstrate how to use multiple and out of order command queues to simultaneously execute multiple kernels on an FPGA.|__Key__ __Concepts__<br> - [Concurrent execution](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/Task-Parallelism)<br> - [Out of Order Command Queues](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/Single-Out-of-Order-Command-Queue)<br> - [Multiple Command Queues](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/Multiple-In-Order-Command-Queues)<br>__Keywords__<br> - [CL_QUEUE_OUT_OF_ORDER_EXEC_MODE_ENABLE](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/Single-Out-of-Order-Command-Queue)<br> - [setCallback](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/Single-Out-of-Order-Command-Queue)
[copy_buffer/][]|This Copy Buffer example demonstrate how one buffer can be copied from another buffer.|__Key__ __Concepts__<br> - Copy Buffer<br>__Keywords__<br> - [cl::CommandQueue](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/Command-Queues)<br> - enqueueCopyBuffer<br> - [enqueueWriteBuffer](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/Buffer-Creation-and-Data-Transfer)<br> - [enqueueReadBuffer](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/Buffer-Creation-and-Data-Transfer)<br> - [enqueueMigrateMemObjects](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/Buffer-Creation-and-Data-Transfer)
[data_transfer/][]|This example illustrates several ways to use the OpenCL API to transfer data to and from the FPGA|__Key__ __Concepts__<br> - [OpenCL Host APIs](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/OpenCL-Programming)<br> - [Data Transfer](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/Buffer-Creation-and-Data-Transfer)<br> - [Write Buffers](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/Buffer-Creation-and-Data-Transfer)<br> - [Read Buffers](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/Buffer-Creation-and-Data-Transfer)<br> - [Map Buffers](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/Buffer-Creation-and-Data-Transfer)<br> - Async Memcpy<br>__Keywords__<br> - [enqueueWriteBuffer](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/Buffer-Creation-and-Data-Transfer)<br> - [enqueueReadBuffer](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/Buffer-Creation-and-Data-Transfer)<br> - [enqueueMapBuffer](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/Buffer-Creation-and-Data-Transfer)<br> - enqueueUnmapMemObject<br> - [enqueueMigrateMemObjects](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/Buffer-Creation-and-Data-Transfer)
[debug_profile/][]|This is simple example of vector addition and printing profile data (wall clock time taken between start and stop). It also dump a waveform file which can be reloaded to vivado to see the waveform. Run command 'vivado -source ./scripts/open_waveform.tcl -tclargs <device_name>-<kernel_name>.<target>.<device_name>.wdb' to launch waveform viewer. User can also update batch to gui in xrt.ini file to see the live waveform while running application.|__Key__ __Concepts__<br> - [Use of Profile API](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/Profiling-the-Application)<br> - [Waveform Dumping and loading](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/Waveform-View-and-Live-Waveform-Viewer)<br>
[device_only_buffer/][]|This example will demonstrate how to create buffers in global memory which are not mapped to host. The device only memory allocation is done through the host code. The kernel can read data from device memory and write result to device memory.|__Key__ __Concepts__<br> - Device only buffer<br>__Keywords__<br> - CL_MEM_HOST_NO_ACCESS
[device_query/][]|This Example prints the OpenCL properties of the platform and its devices using OpenCL CPP APIs. It also displays the limits and capabilities of the hardware.|__Key__ __Concepts__<br> - OpenCL API<br> - Querying device properties<br>
[errors/][]|This example discuss the different reasons for errors in OpenCL and how to handle them at runtime.|__Key__ __Concepts__<br> - OpenCL API<br> - Error handling<br>__Keywords__<br> - CL_SUCCESS<br> - CL_DEVICE_NOT_FOUND<br> - CL_DEVICE_NOT_AVAILABLE
[errors_cpp/][]|This example discuss the different reasons for errors in OpenCL C++ and how to handle them at runtime.|__Key__ __Concepts__<br> - OpenCL Host API<br> - Error handling<br>__Keywords__<br> - CL_SUCCESS<br> - CL_DEVICE_NOT_FOUND<br> - CL_DEVICE_NOT_AVAILABLE<br> - CL_INVALID_VALUE<br> - CL_INVALID_KERNEL_NAME<br> - CL_INVALID_BUFFER_SIZE
[hbm_large_buffers/][]|This is a simple example of vector addition to describe how HBM pseudo-channels can be grouped to handle buffers larger than 256 MB.|__Key__ __Concepts__<br> - [High Bandwidth Memory](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/HBM-Configuration-and-Use)<br> - Multiple HBM Pseudo-channel Groups<br>__Keywords__<br> - [HBM](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/HBM-Configuration-and-Use)
[hbm_rama_ip/][]|This is host application to test HBM interface bandwidth for buffers > 256 MB with pseudo random 1024 bit data access pattern, mimicking Ethereum Ethash workloads. Design contains 4 compute units of Kernel, 2 with and 2 without RAMA IP. Each compute unit reads 1024 bits from a pseudo random address in each of 2 pseudo channel groups and writes the results of a simple mathematical operation to a pseudo random address in 2 other pseudo channel groups. Each buffer is 1 GB large requiring 4 HBM banks. Since the first 2 CUs requires 4 buffers each and are then used again by the other 2 CUs, the .cfg file is allocating the buffers to all the 32 HBM banks.  The host application runs the compute units concurrently to measure the overall bandwidth between kernel and HBM Memory.|__Key__ __Concepts__<br> - [High Bandwidth Memory](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/HBM-Configuration-and-Use)<br> - [Multiple HBM Pseudo-channels](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/HBM-Configuration-and-Use)<br> - Random Memory Access<br> - Linear Feedback Shift Register<br> - [RAMA IP](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/Random-Access-and-the-RAMA-IP)<br>__Keywords__<br> - [HBM](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/HBM-Configuration-and-Use)<br> - [ra_master_interface](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/Random-Access-and-the-RAMA-IP)
[hbm_simple/][]|This is a simple example of vector addition to describe how to use HLS kernels with HBM (High Bandwidth Memory) for achieving high throughput.|__Key__ __Concepts__<br> - [High Bandwidth Memory](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/HBM-Configuration-and-Use)<br> - Multiple HBM pseudo-channels<br>__Keywords__<br> - [HBM](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/HBM-Configuration-and-Use)<br> - [XCL_MEM_TOPOLOGY](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/Assigning-DDR-Bank-in-Host-Code)<br> - [cl_mem_ext_ptr_t](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/Assigning-DDR-Bank-in-Host-Code)<br> - [trace_memory](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/profile-Options)<br> - [trace_buffer_size](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/xrt.ini-File)<br> - [opencl_trace](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/xrt.ini-File)
[host_memory_copy_buffer/][]|This is simple host memory example to describe how host-only memory can be copied to device-only memory and vice-versa.|__Key__ __Concepts__<br> - host memory<br>__Keywords__<br> - XCL_MEM_EXT_HOST_ONLY<br> - CL_MEM_HOST_NO_ACCESS<br> - enqueueCopyBuffer
[host_memory_copy_kernel/][]|This is a Host Memory Example to describe how data can be copied between host-only buffer and device-only buffer using User Copy Kernel.|__Key__ __Concepts__<br> - host memory<br>__Keywords__<br> - XCL_MEM_EXT_HOST_ONLY<br> - CL_MEM_HOST_NO_ACCESS<br> - [enqueueMapBuffer](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/Buffer-Creation-and-Data-Transfer)
[host_memory_simple/][]|This is simple host memory example to describe how a user kernel can access the host memory. The host memory allocation is done through the host code. The kernel reads data from host memory and writes result to host memory.|__Key__ __Concepts__<br> - host memory<br> - address translation unit<br>__Keywords__<br> - XCL_MEM_EXT_HOST_ONLY<br> - HOST[0]
[iops_test/][]|This is simple test design to measure Input/Output Operations per second. In this design, a simple kernel is enqueued many times and measuring overall IOPS.|__Key__ __Concepts__<br> - Input/Output Operations per second<br>__Keywords__<br> - [CL_QUEUE_OUT_OF_ORDER_EXEC_MODE_ENABLE](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/Single-Out-of-Order-Command-Queue)
[mult_compute_units/][]|This is simple Example of Multiple Compute units to showcase how a single kernel can be instantiated into Multiple compute units. Host code will show how to use multiple compute units and run them concurrently.|__Key__ __Concepts__<br> - [Multiple Compute Units](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/Symmetrical-and-Asymmetrical-Compute-Units)<br>__Keywords__<br> - [nk](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/connectivity-Options)
[multiple_cus_asymmetrical/][]|This is simple example of vector addition to demonstrate how to connect each compute unit to different banks and how to use these compute units in host applications|__Key__ __Concepts__<br> - [Multiple Compute Units](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/Symmetrical-and-Asymmetrical-Compute-Units)<br> - [Task Level Parallelism](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/Task-Parallelism)<br>
[overlap/][]|This examples demonstrates techniques that allow user to overlap Host(CPU) and FPGA computation in an application. It will cover asynchronous operations and event object.|__Key__ __Concepts__<br> - OpenCL Host API<br> - [Synchronize Host and FPGA](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/Event-Synchronization)<br> - [Asynchronous Processing](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/Event-Synchronization)<br> - [Events](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/Overlapping-Data-Transfers-with-Kernel-Computation)<br> - [Asynchronous memcpy](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/Event-Synchronization)<br>__Keywords__<br> - [cl_event](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/Event-Synchronization)<br> - [cl::CommandQueue](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/Command-Queues)<br> - [CL_QUEUE_OUT_OF_ORDER_EXEC_MODE_ENABLE](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/Single-Out-of-Order-Command-Queue)<br> - [enqueueMigrateMemObjects](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/Buffer-Creation-and-Data-Transfer)
[p2p_bandwidth/][]|This is simple example to test data transfer between SSD and FPGA.|__Key__ __Concepts__<br> - [P2P](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/p2p)<br> - SmartSSD<br> - XDMA<br>__Keywords__<br> - XCL_MEM_EXT_P2P_BUFFER<br> - pread<br> - pwrite
[p2p_fpga2fpga/][]|This is simple example to explain P2P transfer between two FPGA devices.|__Key__ __Concepts__<br> - [P2P](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/p2p)<br> - Multi-FPGA Execution<br> - XDMA<br>__Keywords__<br> - XCL_MEM_EXT_P2P_BUFFER
[p2p_overlap_bandwidth/][]|This is simple example to test Synchronous and Asyncronous data transfer between SSD and FPGA.|__Key__ __Concepts__<br> - [P2P](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/p2p)<br> - SmartSSD<br> - XDMA<br>__Keywords__<br> - XCL_MEM_EXT_P2P_BUFFER<br> - pread<br> - pwrite
[p2p_simple/][]|This is simple example of vector increment to describe P2P between FPGA and NVMe SSD.|__Key__ __Concepts__<br> - [P2P](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/p2p)<br> - NVMe SSD<br> - SmartSSD<br>__Keywords__<br> - XCL_MEM_EXT_P2P_BUFFER<br> - pread<br> - pwrite<br> - O_DIRECT<br> - O_RDWR
[streaming_free_running_k2k/][]|This is simple example which demonstrate how to use and configure a free running kernel.|__Key__ __Concepts__<br> - [Free Running Kernel](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/Free-Running-Kernel)<br>__Keywords__<br> - [ap_ctrl_none](https://docs.xilinx.com/r/en-US/ug1399-vitis-hls/Block-Level-Control-Protocols)<br> - [stream_connect](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/Specifying-Streaming-Connections-between-Compute-Units)
[streaming_k2k_mm/][]|This is a simple kernel to kernel streaming Vector Add and Vector Multiply C Kernel design with 2 memory mapped input to kernel 1, 1 Stream output from kernel 1 to input of kernel 2, 1 memory mapped input to kernel 2, and 1 memory mapped output that demonstrates on how to process a stream of data for computation between two kernels. This design also illustrates how to set FIFO depth for AXIS connections i.e. for the stream connecting the two kernels|__Key__ __Concepts__<br> - [Read/Write Stream](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/Host-Coding-for-Free-Running-Kernels)<br> - [Create/Release Stream](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/Host-Coding-for-Free-Running-Kernels)<br> - [AXIS FIFO depth](https://docs.xilinx.com/r/en-US/ug1399-vitis-hls/Specifying-Compiler-Created-FIFO-Depth)<br>__Keywords__<br> - [stream_connect](https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/Specifying-Streaming-Connections-between-Compute-Units)

[.]:.
[concurrent_kernel_execution/]:concurrent_kernel_execution/
[copy_buffer/]:copy_buffer/
[data_transfer/]:data_transfer/
[debug_profile/]:debug_profile/
[device_only_buffer/]:device_only_buffer/
[device_query/]:device_query/
[errors/]:errors/
[errors_cpp/]:errors_cpp/
[hbm_large_buffers/]:hbm_large_buffers/
[hbm_rama_ip/]:hbm_rama_ip/
[hbm_simple/]:hbm_simple/
[host_memory_copy_buffer/]:host_memory_copy_buffer/
[host_memory_copy_kernel/]:host_memory_copy_kernel/
[host_memory_simple/]:host_memory_simple/
[iops_test/]:iops_test/
[mult_compute_units/]:mult_compute_units/
[multiple_cus_asymmetrical/]:multiple_cus_asymmetrical/
[overlap/]:overlap/
[p2p_bandwidth/]:p2p_bandwidth/
[p2p_fpga2fpga/]:p2p_fpga2fpga/
[p2p_overlap_bandwidth/]:p2p_overlap_bandwidth/
[p2p_simple/]:p2p_simple/
[streaming_free_running_k2k/]:streaming_free_running_k2k/
[streaming_k2k_mm/]:streaming_k2k_mm/
